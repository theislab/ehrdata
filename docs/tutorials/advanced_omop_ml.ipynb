{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on OMOP Data with PyPOTS\n",
    "\n",
    "This tutorial demonstrates how to quickly apply machine learning to OMOP data using [PyPOTS](https://github.com/WenjieDu/PyPOTS), a powerful toolkit for time series analysis.\n",
    "\n",
    "**Prerequisites:** Complete the [OMOP Introduction tutorial](omop_tables_tutorial.ipynb) first to understand how to load OMOP data into EHRData.\n",
    "\n",
    "## Use Case: ICU Mortality Prediction\n",
    "\n",
    "We'll predict in-hospital mortality for ICU patients using the [MIMIC-IV demo dataset in OMOP format](https://physionet.org/content/mimic-iv-demo-omop/0.9/).\n",
    "\n",
    "```{note}\n",
    "This is a demonstration example. Real clinical prediction requires more sophisticated preprocessing, validation, and careful consideration of clinical context.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypots ehrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPOTS requires this for scipy compatibility\n",
    "import os\n",
    "\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
    "\n",
    "import ehrdata as ed\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import torch\n",
    "from pypots.classification import BRITS\n",
    "from pypots.imputation import SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Database and Download Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connection\n",
    "con = duckdb.connect(\":memory:\")\n",
    "\n",
    "# Download MIMIC-IV OMOP demo data\n",
    "ed.dt.mimic_iv_omop(backend_handle=con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Cohort\n",
    "\n",
    "We'll focus on ICU patients by filtering `visit_occurrence` for ICU stays using OMOP concept IDs:\n",
    "- **4305366**: Surgical ICU\n",
    "- **40481392**: Medical ICU\n",
    "- **32037**: Intensive Care\n",
    "- **763903**: Trauma ICU\n",
    "- **4149943**: Cardiac ICU\n",
    "\n",
    "If a patient had multiple ICU stays, we select their **first ICU visit**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for first ICU visit per patient\n",
    "con.execute(\"\"\"\n",
    "    WITH RankedVisits AS (\n",
    "        SELECT\n",
    "            v.*,\n",
    "            vd.*,\n",
    "            ROW_NUMBER() OVER (PARTITION BY v.person_id ORDER BY v.visit_start_date) AS rn\n",
    "        FROM visit_occurrence v\n",
    "        JOIN visit_detail vd USING (visit_occurrence_id)\n",
    "        WHERE vd.visit_detail_concept_id IN (4305366, 40481392, 32037, 763903, 4149943)\n",
    "    ),\n",
    "    first_icu_visit_occurrence_id AS (\n",
    "        SELECT visit_occurrence_id\n",
    "        FROM RankedVisits\n",
    "        WHERE rn = 1\n",
    "    )\n",
    "    DELETE FROM visit_occurrence\n",
    "    WHERE visit_occurrence_id NOT IN (SELECT visit_occurrence_id FROM first_icu_visit_occurrence_id)\n",
    "\"\"\")\n",
    "\n",
    "# Check how many ICU visits remain\n",
    "n_visits = con.execute(\"SELECT COUNT(*) FROM visit_occurrence\").fetchone()[0]\n",
    "print(f\"ICU cohort: {n_visits} patients (first ICU visit only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build EHRData from OMOP\n",
    "\n",
    "Now we construct the EHRData object using **ICU visit start** as the time reference (t=0) for each patient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup observations from person + visit_occurrence\n",
    "edata = ed.io.omop.setup_obs(\n",
    "    backend_handle=con,\n",
    "    observation_table=\"person_visit_occurrence\",  # Each row = one ICU visit\n",
    ")\n",
    "\n",
    "print(f\"Created EHRData with {edata.n_obs} ICU visits\")\n",
    "edata.obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract measurements from the first 24 hours\n",
    "edata = ed.io.omop.setup_variables(\n",
    "    edata=edata,\n",
    "    backend_handle=con,\n",
    "    layer=\"measurements\",\n",
    "    data_tables=[\"measurement\"],\n",
    "    data_field_to_keep={\"measurement\": \"value_as_number\"},\n",
    "    interval_length_number=1,\n",
    "    interval_length_unit=\"h\",  # Hourly intervals\n",
    "    num_intervals=24,  # First 24 hours\n",
    "    aggregation_strategy=\"last\",\n",
    "    enrich_var_with_feature_info=True,\n",
    "    instantiate_tensor=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal shape: {edata.n_obs} ICU visits × {edata.n_vars} variables × {edata.n_tem} hours\")\n",
    "edata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for PyPOTS\n",
    "\n",
    "Extract the time series tensor - PyPOTS works directly with EHRData's `.layers` format!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time series data: (n_patients, n_variables, n_timepoints)\n",
    "X = edata.layers[\"measurements\"]\n",
    "\n",
    "print(f\"Time series shape: {X.shape}\")\n",
    "print(f\"Missing values: {np.isnan(X).sum()} / {X.size} ({np.isnan(X).mean() * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Imputation with SAITS\n",
    "\n",
    "Clinical ICU data has many missing values. Let's use SAITS (Self-Attention-based Imputation) to intelligently fill them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAITS imputer\n",
    "torch.manual_seed(42)\n",
    "saits = SAITS(\n",
    "    n_steps=X.shape[2],\n",
    "    n_features=X.shape[1],\n",
    "    n_layers=2,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    d_k=32,\n",
    "    d_v=32,\n",
    "    d_ffn=64,\n",
    "    dropout=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "# Fit and impute\n",
    "saits.fit({\"X\": X})\n",
    "imputed_result = saits.impute({\"X\": X})\n",
    "\n",
    "print(f\"\\nImputed data shape: {imputed_result['imputation'].shape}\")\n",
    "print(f\"Remaining missing: {np.isnan(imputed_result['imputation']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Mortality Prediction with BRITS\n",
    "\n",
    "Now let's predict in-hospital mortality using BRITS, which handles missing values during classification.\n",
    "\n",
    "First, prepare the labels from OMOP's `death` table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get death information from the database\n",
    "death_df = con.execute(\"SELECT person_id, death_date FROM death\").df()\n",
    "\n",
    "# Merge with our cohort to get mortality labels\n",
    "obs_with_death = edata.obs.merge(death_df, on=\"person_id\", how=\"left\", suffixes=(\"\", \"_death\"))\n",
    "\n",
    "# Create binary labels (1 = died, 0 = survived)\n",
    "y = (~obs_with_death[\"death_date_death\"].isna()).astype(int).values\n",
    "\n",
    "print(f\"Mortality rate: {y.mean() * 100:.1f}%\")\n",
    "print(f\"Deaths: {y.sum()} / {len(y)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test (simple split for demonstration)\n",
    "n_train = int(0.7 * len(X))\n",
    "\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training set: {len(X_train)} patients ({y_train.mean() * 100:.1f}% mortality)\")\n",
    "print(f\"Test set: {len(X_test)} patients ({y_test.mean() * 100:.1f}% mortality)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BRITS classifier\n",
    "torch.manual_seed(42)\n",
    "brits = BRITS(n_steps=X.shape[2], n_features=X.shape[1], rnn_hidden_size=64, n_classes=2, epochs=20, batch_size=16)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training BRITS...\")\n",
    "brits.fit({\"X\": X_train, \"y\": y_train})\n",
    "\n",
    "# Make predictions\n",
    "predictions = brits.predict({\"X\": X_test})\n",
    "pred_labels = predictions[\"classification\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (pred_labels == y_test).mean()\n",
    "print(f\"\\nTest Accuracy: {accuracy * 100:.1f}%\")\n",
    "print(f\"Baseline (predict majority class): {max(y_test.mean(), 1 - y_test.mean()) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we learned:\n",
    "\n",
    "- ✅ How to define clinical cohorts using SQL queries on OMOP tables\n",
    "- ✅ How to use `person_visit_occurrence` for visit-level analysis\n",
    "- ✅ How to extract ICU time series with hourly resolution\n",
    "- ✅ How to apply PyPOTS models to OMOP data:\n",
    "  - **SAITS** for intelligent missing value imputation\n",
    "  - **BRITS** for mortality prediction with built-in imputation\n",
    "- ✅ The complete workflow: **OMOP cohort** → **EHRData** → **PyPOTS ML**\n",
    "\n",
    "## Key Advantages of This Approach\n",
    "\n",
    "1. **OMOP Standardization** - Same code works across different hospitals\n",
    "2. **Cohort Definition** - SQL queries filter for specific patient populations\n",
    "3. **Time-Aware Analysis** - Visit start times define temporal reference (t=0)\n",
    "4. **Missing Data Handling** - PyPOTS handles incomplete clinical data natively\n",
    "5. **Rapid Prototyping** - From database to predictions in minimal code\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different cohorts: chronic disease patients, emergency admissions, etc.\n",
    "- Experiment with different time windows and aggregation strategies\n",
    "- Use other PyPOTS models: `CRLI` (clustering), `Transformer` (forecasting)\n",
    "- Combine multiple OMOP tables: measurements + drugs + procedures\n",
    "- See the [PyPOTS tutorial](tutorial_time_series_with_pypots.ipynb) for more advanced features\n",
    "- Explore the [OHDSI Book](https://ohdsi.github.io/TheBookOfOhdsi/) for OMOP best practices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on OMOP Data with PyPOTS\n",
    "\n",
    "This tutorial demonstrates how to quickly apply machine learning to OMOP data using [PyPOTS](https://github.com/WenjieDu/PyPOTS), a powerful toolkit for time series analysis.\n",
    "\n",
    "**Prerequisites:** Complete the [OMOP Introduction tutorial](omop_tables_tutorial.ipynb) first to understand how to load OMOP data into EHRData.\n",
    "\n",
    "## What is PyPOTS?\n",
    "\n",
    "PyPOTS provides state-of-the-art neural network models for time series tasks:\n",
    "- **Imputation** - Fill missing values in incomplete time series\n",
    "- **Classification** - Predict outcomes from time series\n",
    "- **Forecasting** - Predict future values\n",
    "- **Clustering** - Group similar patients\n",
    "\n",
    "PyPOTS works seamlessly with EHRData objects!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypots ehrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPOTS requires this for scipy compatibility\n",
    "import os\n",
    "\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
    "\n",
    "import ehrdata as ed\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import torch\n",
    "from pypots.classification import BRITS\n",
    "from pypots.imputation import SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load OMOP Data\n",
    "\n",
    "We'll use the same MIMIC-IV OMOP demo dataset from the introduction tutorial:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup database and download data\n",
    "con = duckdb.connect(\":memory:\")\n",
    "data_path = ed.dt.mimic_iv_omop(backend_handle=con)\n",
    "\n",
    "# Setup observations\n",
    "edata = ed.io.omop.setup_obs(backend_handle=con, observation_table=\"person\", death_table=True)\n",
    "\n",
    "print(f\"Loaded {edata.n_obs} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables - extract time series measurements\n",
    "edata = ed.io.omop.setup_variables(\n",
    "    edata=edata,\n",
    "    backend_handle=con,\n",
    "    layer=\"measurements\",\n",
    "    data_tables=[\"measurement\"],\n",
    "    data_field_to_keep={\"measurement\": \"value_as_number\"},\n",
    "    interval_length_number=1,\n",
    "    interval_length_unit=\"day\",\n",
    "    num_intervals=7,  # First week of data\n",
    "    aggregation_strategy=\"mean\",\n",
    "    enrich_var_with_feature_info=True,\n",
    ")\n",
    "\n",
    "print(f\"Final shape: {edata.n_obs} patients × {edata.n_vars} variables × {edata.n_tem} days\")\n",
    "edata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for PyPOTS\n",
    "\n",
    "PyPOTS expects data in a specific format. EHRData's `.layers` attribute is already compatible!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time series data\n",
    "# Shape: (n_patients, n_variables, n_timepoints)\n",
    "X = edata.layers[\"measurements\"]\n",
    "\n",
    "print(f\"Time series shape: {X.shape}\")\n",
    "print(f\"Missing values: {np.isnan(X).sum()} / {X.size} ({np.isnan(X).mean() * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Imputation with SAITS\n",
    "\n",
    "Clinical data has many missing values. SAITS (Self-Attention-based Imputation) can fill them intelligently:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Mortality Prediction with BRITS\n",
    "\n",
    "Now let's predict in-hospital mortality using BRITS (Bidirectional Recurrent Imputation for Time Series), which handles missing values during classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels from death information in .obs\n",
    "y = (~edata.obs[\"death_date\"].isna()).astype(int).values\n",
    "\n",
    "print(f\"Mortality rate: {y.mean() * 100:.1f}%\")\n",
    "print(f\"Deaths: {y.sum()} / {len(y)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (simple split for demonstration)\n",
    "n_train = int(0.7 * len(X))\n",
    "\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training set: {len(X_train)} patients\")\n",
    "print(f\"Test set: {len(X_test)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BRITS classifier\n",
    "brits = BRITS(n_steps=X.shape[2], n_features=X.shape[1], rnn_hidden_size=128, n_classes=2, epochs=10, batch_size=32)\n",
    "\n",
    "# Train the model\n",
    "brits.fit({\"X\": X_train, \"y\": y_train})\n",
    "\n",
    "# Make predictions\n",
    "predictions = brits.predict({\"X\": X_test})\n",
    "pred_labels = predictions[\"classification\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (pred_labels == y_test).mean()\n",
    "print(f\"Test Accuracy: {accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we learned:\n",
    "\n",
    "- ✅ How to apply PyPOTS models to OMOP data loaded via ehrdata\n",
    "- ✅ **Imputation**: Using SAITS to fill missing values in clinical time series\n",
    "- ✅ **Classification**: Using BRITS to predict mortality while handling missing data\n",
    "- ✅ The seamless workflow: **OMOP data** → **EHRData** → **PyPOTS models**\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**Why this workflow is powerful:**\n",
    "1. **OMOP standardization** - Your code works across different hospitals' data\n",
    "2. **EHRData structure** - Clean 3D tensor format (patients × variables × time)\n",
    "3. **PyPOTS integration** - State-of-the-art models work directly with EHRData\n",
    "4. **Minimal code** - From database to predictions in ~20 lines\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try other PyPOTS models: `CRLI` for clustering, `Transformer` for forecasting\n",
    "- Use different OMOP tables: `drug_exposure`, `procedure_occurrence`\n",
    "- Apply to your own OMOP database following the same workflow\n",
    "- See the [PyPOTS documentation](https://docs.pypots.com/) for more models\n",
    "- Explore the [full PyPOTS tutorial](tutorial_time_series_with_pypots.ipynb) for advanced features\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
